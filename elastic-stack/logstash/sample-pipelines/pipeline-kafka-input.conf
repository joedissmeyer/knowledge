input {
  # Primary kafka input. 
  # Uses the json codec. If plain text data use "plain" instead.
  kafka {
    id => "primary_kafka_input"
    bootstrap_servers => "PRIMARY-KAFKA1:9092,PRIMARY-KAFKA2:9092,PRIMARY-KAFKA3:9092"
    topics => ["my-topic"]
    enable_auto_commit => "true"
    #codec => "plain"
    codec => "json"
  }
  # Add a 2nd kafka input for an additional instance. Can be removed if not needed.
  # Uses the json codec. If plain text data use "plain" instead.
  kafka {
    id => "secondary_kafka_input"
    bootstrap_servers => "SECOND-KAFKA1:9092,SECOND-KAFKA2:9092,SECOND-KAFKA3:9092"
    topics => ["my-topic"]
    enable_auto_commit => "true"
    #codec => "plain"
    codec => "json"
  }
}

filter {
  # Sample starter grok pattern
  grok {
    match => { "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] \[%{LOGLEVEL:loglevel}\] - %{GREEDYDATA:json_payload}"}
  }
  # Final pass. Drop redundant data
  mutate {
    remove_field => [ "message" ]
  }
}

output {
  elasticsearch {
   hosts => ["elastic-ingest-node:9200"]
   # include document_type of _doc for ELK v6. Not needed for ELK v7
   # document_type => "_doc"
   index => "microservices"
   user => "logstash-user"
   password => "BogusPassword123"
 }
}