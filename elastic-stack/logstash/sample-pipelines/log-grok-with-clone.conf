input {
  beats {
    port => 5044
  }
}

filter {
  # Drop bogus messages.
  if "bogus message" in [message] {
    drop{}
  }

  grok {
    match => { "message" => "%{NUMBER:loghour}:%{NUMBER:logminute}:%{NUMBER:logsecond} %{NOTSPACE:terminalId} %{NOTSPACE:operationId}:%{NOTSPACE:logId} %{GREEDYDATA:logMessage}"}
  }
  
  #Example grok for extracting fields from a file name, from right to left.
  grok {
    match => { "source" => "_%{YEAR:logyear}_%{MONTHNUM:logmonth}_%{MONTHDAY:logday}_%{NOTSPACE:loghost}.txt$" }
  }
  mutate {
    add_field => {"logdatetime" => "%{logyear}-%{logmonth}-%{logday} %{loghour}:%{logminute}:%{logsecond}"}
    add_field => { "pipeline_id" => "first_index" }
  }

  date {
    match => [ "logdatetime", "YYYY-MM-dd HH:mm:ss.SSS" ]
    timezone => "America/New_York"
  }

  # Get the transactionId and start time of the transaction if available in the message. Store in variables.
  if "first search term" in [logMessage] {
    mutate {
      replace => { "pipeline_id" => "transaction-index" }
    }

    # Use gsub to remove new lines and line returns in the log entry
    mutate {
      gsub => [ "logMessage", "\r\n", "" ]
    }
    

    grok {
      match => { "logMessage" => "^<\?xml version='1.0' standalone='yes'\?><Tx%{NOTSPACE:transactionId}In>%{GREEDYDATA:logData}"}
    }

    mutate {
      add_field => { "transaction_start" => "%{logdatetime}" }
    }
    date {
      match => [ "transaction_start", "YYYY-MM-dd HH:mm:ss.SSS" ]
      timezone => "Etc/UTC"
      target => "transaction_start"
    }

    clone {
      remove_field => [ "message", "offset", "input_type", "source" ]
      clones => [ "transactions" ]
    }

  }

  # Get the transactionId and end time of the transaction if available in the message. Store in variables.
  if "second search term" in [logMessage] {
    mutate {
      replace => { "pipeline_id" => "transaction-index" }
    }
    mutate {
      gsub => [ "logMessage", "\r\n", "" ]
    }

    grok {
      match => { "logMessage" => "EFTPOS response : <Transaction> +<TX%{NOTSPACE:transactionId}OUT%{GREEDYDATA:logData}"}
    }
    mutate {
      add_field => { "return_code" => "declined" }
    }

    if "Approved" in [message] {
      mutate {
        replace => { "return_code" => "approved" }
      }
    }

    mutate {
      add_field => { "transaction_end" => "%{logdatetime}" }
    }
    date {
      match => [ "transaction_end", "YYYY-MM-dd HH:mm:ss.SSS" ]
      timezone => "America/New_York"
      target => "transaction_end"
    }

    clone {
      remove_field => [ "message", "offset", "input_type", "source" ]
      clones => [ "transactions" ]
    }
  }

  mutate {
    remove_field => [ "logMessage", "logData", "operationId" ]
  }
}

output {
  if [pipeline_id] == "first_index" {
    elasticsearch {
      hosts => ["elastic-ingest:9200"]
      index => "first_index"
      user => "logstash-user"
      password => "BogusPassword123"
    }
  } else if [pipeline_id] == "transaction-index" {
      elasticsearch {
        hosts => ["elastic-ingest:9200"]
        index => "transaction-index"
        action => "update"
        document_id => "%{transactionId}"
        doc_as_upsert => true
        user => "logstash-user"
        password => "BogusPassword123"
      }
    }
}